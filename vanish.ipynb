{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Vanish - Face Anonymization\n",
        "\n",
        "Detect and blur faces using Florence-2 + SAM2.\n",
        "\n",
        "- **Florence-2**: Object detection to find human faces\n",
        "- **SAM2**: Precise segmentation for clean masking\n",
        "- **Pixelation**: Apply blur effect to masked regions\n",
        "\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q flash_attn timm accelerate einops supervision\n",
        "\n",
        "# Download SAM2 checkpoints\n",
        "!mkdir -p models/sam2\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt -P models/sam2\n",
        "\n",
        "# Install SAM2\n",
        "!git clone -q https://github.com/facebookresearch/segment-anything-2.git\n",
        "%cd segment-anything-2\n",
        "!pip install -q -e .\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Florence-2\n",
        "from transformers import AutoModelForCausalLM, AutoProcessor\n",
        "import torch\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Florence-2-large-ft\",\n",
        "    device_map=\"cuda\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\n",
        "    \"microsoft/Florence-2-large-ft\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(\"✓ Florence-2 loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core functions\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from sam2.build_sam import build_sam2\n",
        "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
        "\n",
        "\n",
        "def find_all_faces(image):\n",
        "    \"\"\"Find all human faces using Florence-2 object detection.\"\"\"\n",
        "    prompt = \"<OD>\"\n",
        "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(DEVICE)\n",
        "    \n",
        "    with torch.inference_mode():\n",
        "        generated_ids = model.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            pixel_values=inputs[\"pixel_values\"],\n",
        "            max_new_tokens=2048,\n",
        "            do_sample=False,\n",
        "        )\n",
        "    \n",
        "    text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
        "    results = processor.post_process_generation(text, task=\"<OD>\", image_size=(image.width, image.height))\n",
        "    \n",
        "    faces = [bbox for bbox, label in zip(results[\"<OD>\"][\"bboxes\"], results[\"<OD>\"][\"labels\"]) if label == \"human face\"]\n",
        "    return faces\n",
        "\n",
        "\n",
        "def find_main_speakers(image):\n",
        "    \"\"\"Find main speaker faces to exclude from blurring.\"\"\"\n",
        "    prompt = \"<CAPTION_TO_PHRASE_GROUNDING> human face (main speaker)\"\n",
        "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(DEVICE)\n",
        "    \n",
        "    with torch.inference_mode():\n",
        "        generated_ids = model.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            pixel_values=inputs[\"pixel_values\"],\n",
        "            max_new_tokens=2048,\n",
        "            do_sample=False,\n",
        "        )\n",
        "    \n",
        "    text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
        "    results = processor.post_process_generation(text, task=\"<CAPTION_TO_PHRASE_GROUNDING>\", image_size=(image.width, image.height))\n",
        "    \n",
        "    speakers = [bbox for bbox, label in zip(results[\"<CAPTION_TO_PHRASE_GROUNDING>\"][\"bboxes\"], results[\"<CAPTION_TO_PHRASE_GROUNDING>\"][\"labels\"]) if label == \"human face\"]\n",
        "    return speakers\n",
        "\n",
        "\n",
        "def is_overlapping(box1, box2, threshold=0.7):\n",
        "    \"\"\"Check if two boxes overlap significantly.\"\"\"\n",
        "    x1_min, y1_min, x1_max, y1_max = box1\n",
        "    x2_min, y2_min, x2_max, y2_max = box2\n",
        "    \n",
        "    x_overlap = max(0, min(x1_max, x2_max) - max(x1_min, x2_min))\n",
        "    y_overlap = max(0, min(y1_max, y2_max) - max(y1_min, y2_min))\n",
        "    overlap_area = x_overlap * y_overlap\n",
        "    \n",
        "    area1 = (x1_max - x1_min) * (y1_max - y1_min)\n",
        "    area2 = (x2_max - x2_min) * (y2_max - y2_min)\n",
        "    \n",
        "    return overlap_area >= threshold * min(area1, area2)\n",
        "\n",
        "\n",
        "def find_passerby_faces(image, exclude_speakers=True):\n",
        "    \"\"\"Find faces to blur (all faces minus main speakers).\"\"\"\n",
        "    all_faces = find_all_faces(image)\n",
        "    \n",
        "    if not exclude_speakers:\n",
        "        return all_faces\n",
        "    \n",
        "    speakers = find_main_speakers(image)\n",
        "    return [face for face in all_faces if not any(is_overlapping(face, s) for s in speakers)]\n",
        "\n",
        "\n",
        "def pixelate_region(image, masks, pixel_size=10):\n",
        "    \"\"\"Apply pixelation to masked regions.\"\"\"\n",
        "    masks = masks.astype(bool)\n",
        "    height, width = image.shape[:2]\n",
        "    result = image.copy()\n",
        "    \n",
        "    for y in range(0, height, pixel_size):\n",
        "        for x in range(0, width, pixel_size):\n",
        "            y_end = min(y + pixel_size, height)\n",
        "            x_end = min(x + pixel_size, width)\n",
        "            block = image[y:y_end, x:x_end]\n",
        "            \n",
        "            combined_mask = np.zeros(block.shape[:2], dtype=bool)\n",
        "            for mask in masks:\n",
        "                combined_mask |= mask[y:y_end, x:x_end]\n",
        "            \n",
        "            if combined_mask.any():\n",
        "                avg_color = [int(np.mean(c[combined_mask])) for c in cv2.split(block)]\n",
        "                for c in range(3):\n",
        "                    block[:, :, c][combined_mask] = avg_color[c]\n",
        "                result[y:y_end, x:x_end] = block\n",
        "    \n",
        "    return result\n",
        "\n",
        "\n",
        "print(\"✓ Functions defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load SAM2\n",
        "SAM2_CHECKPOINT = \"models/sam2/sam2_hiera_large.pt\"\n",
        "SAM2_CONFIG = \"sam2_hiera_l.yaml\"\n",
        "\n",
        "sam2_model = build_sam2(SAM2_CONFIG, SAM2_CHECKPOINT, device=DEVICE, apply_postprocessing=False)\n",
        "sam2_predictor = SAM2ImagePredictor(sam2_model)\n",
        "\n",
        "print(\"✓ SAM2 loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main vanish function\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def vanish(image_path, pixel_size=10, exclude_speakers=True, show=True):\n",
        "    \"\"\"\n",
        "    Detect and pixelate faces in an image.\n",
        "    \n",
        "    Args:\n",
        "        image_path: Path to input image\n",
        "        pixel_size: Size of pixelation blocks\n",
        "        exclude_speakers: If True, don't blur main speakers\n",
        "        show: Display the result\n",
        "    \n",
        "    Returns:\n",
        "        Pixelated image as numpy array\n",
        "    \"\"\"\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    \n",
        "    # Find faces to blur\n",
        "    faces = find_passerby_faces(image, exclude_speakers=exclude_speakers)\n",
        "    print(f\"Found {len(faces)} face(s) to blur\")\n",
        "    \n",
        "    if not faces:\n",
        "        return np.array(image)\n",
        "    \n",
        "    # Segment with SAM2\n",
        "    sam2_predictor.set_image(image)\n",
        "    masks, scores, logits = sam2_predictor.predict(box=faces, multimask_output=False)\n",
        "    masks = np.squeeze(masks)\n",
        "    if masks.ndim == 2:\n",
        "        masks = np.expand_dims(masks, axis=0)\n",
        "    \n",
        "    # Pixelate\n",
        "    image_array = np.array(image)\n",
        "    result = pixelate_region(image_array, masks, pixel_size)\n",
        "    \n",
        "    if show:\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
        "        axes[0].imshow(image)\n",
        "        axes[0].set_title(\"Original\")\n",
        "        axes[0].axis(\"off\")\n",
        "        axes[1].imshow(result)\n",
        "        axes[1].set_title(\"Vanished\")\n",
        "        axes[1].axis(\"off\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    return result\n",
        "\n",
        "\n",
        "print(\"✓ vanish() ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Process an Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process an image\n",
        "# result = vanish(\"your_image.jpg\", pixel_size=10)\n",
        "\n",
        "# To blur ALL faces (including main speakers):\n",
        "# result = vanish(\"your_image.jpg\", pixel_size=10, exclude_speakers=False)\n",
        "\n",
        "# Save result:\n",
        "# Image.fromarray(result).save(\"output.jpg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Process a Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def vanish_video(input_path, output_path, pixel_size=10, exclude_speakers=True):\n",
        "    \"\"\"Process a video frame by frame.\"\"\"\n",
        "    cap = cv2.VideoCapture(input_path)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    \n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "    \n",
        "    print(f\"Processing {total} frames...\")\n",
        "    \n",
        "    frame_num = 0\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        \n",
        "        frame_num += 1\n",
        "        print(f\"\\rFrame {frame_num}/{total}\", end=\"\")\n",
        "        \n",
        "        # Convert BGR to RGB\n",
        "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        pil_frame = Image.fromarray(frame_rgb)\n",
        "        \n",
        "        # Find and segment faces\n",
        "        faces = find_passerby_faces(pil_frame, exclude_speakers=exclude_speakers)\n",
        "        \n",
        "        if faces:\n",
        "            sam2_predictor.set_image(pil_frame)\n",
        "            masks, _, _ = sam2_predictor.predict(box=faces, multimask_output=False)\n",
        "            masks = np.squeeze(masks)\n",
        "            if masks.ndim == 2:\n",
        "                masks = np.expand_dims(masks, axis=0)\n",
        "            frame_rgb = pixelate_region(frame_rgb, masks, pixel_size)\n",
        "        \n",
        "        # Convert back to BGR and write\n",
        "        out.write(cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR))\n",
        "    \n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(f\"\\n✓ Saved to {output_path}\")\n",
        "\n",
        "\n",
        "# Usage:\n",
        "# vanish_video(\"input.mp4\", \"output.mp4\", pixel_size=10)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
